# 3 - Training a deep network to recognize objects

The ground-truth masks created by module 1 can be used to train a deep network to recognize objects. We will take advantage of a pre-trained version of [MobileNet](https://arxiv.org/abs/1704.04861), transferring the learning already achieved in its lower layers to our new task.

This module involves several steps. You will need to:
1. Prepare data sets and a network-training workspace.
2. Install the TensorFlow Object Detection (TFOD) API.
3. Download pre-trained models from the TensorFlow Model Zoo.
4. Configure the training job.
5. Train the network.
6. Evaluate your trained network.
7. Export a trained network.

Before starting on any of these tasks, you may need to clean up your enactments, removing color-map artifacts and objects that "leak" through gaps in the environment mesh.

## Assumptions

### Each enactment is expected to be a directory with the following structure:
```
./EnactmentName
    |
    |--- /GT                <--- Built by module 1
    |--- /Props
    |--- /Subprops
    |--- /Users
    |      |
    |      `--- /UserName
    |              |
    |              |--- Head.fvr
    |              |--- LeftHand.fvr
    |              |--- RightHand.fvr
    |              `--- /POV
    |                     |
    |                     |--- CameraIntrinsics.fvr
    |                     |--- SubpropColorMap.fvr
    |                     |--- /ColorMapCameraFrames
    |                     |--- /DepthMapCameraFrames
    |                     `--- /NormalViewCameraFrames
    |--- Labels.fvr
    `--- metadata.fvr
```
### Your working directory contains your enactments.

Each enactment will have both a directory described above and a corresponding `*_props.txt` file:
```
./MyWorkingDirectory
    |
    |--- /Enactment1
    |--- /Enactment2
   ...
    |--- /EnactmentN
    |--- Enactment1_props.txt
    |--- Enactment2_props.txt
   ...
    |
    `--- EnactmentN_props.txt
```

## 3.0 - Enactment clean-up (if applicable or desirable)

### edit_mask_artifacts.py

Use an interactive sub-program to clean up the mask artifacts. You must specify an enactment and an instance within that enactment. This script will then iterate over all masks and frames containing this instance and allow you to inspect/edit mask artifacts for that instance.

All changes are saved to an `*.editlist` file.

### refine-dataset/Makefile

This make-file builds the sub-programs called by the script above.

## 3.1 - Prepare training and validation sets, and prepare a training workspace

### build_object_recog_dataset.py

Build a dataset from enactments in preparation for training an object-detection network. Several objects typically appear in a single video frame. This script attempts to balance training and validation sets according to the given parameters, but must allocate frames entirely to one set or the other. We do not want to incur false false-positives!

This script creates `training-set.txt`, `validation-set.txt`, `recognizable_objects.txt`, and `recommended_weights.txt`.

### convert_object_recog_dataset.py

Convert the training and validation set files to formats expected by the TensorFlow Object-Detection API. This means creating two folders in the current working directory: `./training/images/train` and `./training/images/test`. This script will also generate `./training/annotations/label_map.pbtxt`. This script expects to find `./training-set.txt` and `./validation-set.txt`. It also expects to find the enactments referenced by these documents. This script will copy all images into the respective folders and generate one annotation XML per image. A single XML contains all detections in its frame.

If you know that one of your classes will be under-represented in the dataset, you up-weight their instances in the training set (actually, you down-weight the others). The file `recommended_weights.txt` generated by `build_object_recog_dataset.py` can be called upon for this purpose when running `convert_object_recog_dataset.py`.

By the end of this process, your working directory should resemble this:

```
./MyWorkingDirectory
    |
    |--- /Enactment1
    |--- /Enactment2
   ...
    |--- /EnactmentN
    |--- Enactment1_props.txt
    |--- Enactment2_props.txt
   ...
    |--- EnactmentN_props.txt
   ...
    |
    `--- /training
           |
           |--- /annotations
           |       |
           |       `--- label_map.pbtxt
           |--- /exported-models     <--- Initially empty
           |--- /images
           |       |
           |       |--- /train       <--- Enactment frames *copied* (not moved)
           |       |       |              from your enactments, and one *.xml per frame.
           |       |       |--- 1.png
           |       |       |--- 1.xml
           |       |       |--- 2.png
           |       |       |--- 2.xml
           |       |      ...
           |       |
           |       `--- /test        <--- Enactment frames *copied* (not moved)
           |               |              from your enactments, and one *.xml per frame.
           |               |--- 1.png
           |               |--- 1.xml
           |               |--- 2.png
           |               |--- 2.xml
           |              ...
           |
           |--- /models              <--- Initially empty
           `--- /pre-trained-models  <--- Initially empty
```

## 3.2 - Install the TensorFlow Object Detection (TFOD) API

Make sure that you have `git` and `protoc` installed:
```
git --version
protoc --version
```

`git` fetches repositories from GitHub, and `protoc` is the Protocol Buffer Compiler. Protocol buffers, or "protobufs" are a binary file format developed by Google and used by the TensorFlow TFRecord type. We will need to compile our training and validation sets into this format so that the TFOD can read them.

Make the following call to clone the TensorFlow Object-Detection API repository.
```
git clone https://github.com/tensorflow/models.git
```

Now, from your current working directory, make the following calls:
```
cd models/research
protoc object_detection/protos/*.proto --python_out=.
git clone https://github.com/cocodataset/cocoapi.git
cd cocoapi/PythonAPI
make
```

If the call to `make` fails, open the `Makefile` in a text editor and be sure that it calls the correct version of Python. You may need to change `python` in lines 3 and 8 to `python3` or `python3.6`.

Moving on:

```
cp -r pycocotools ../..
cd ../..
cp object_detection/packages/tf2/setup.py .
python3.6 -m pip install .
```

You may receive some errors after this last command, which you can likely ignore. The real test of whether this API installation succeeded is the following:
```
python3.6 object_detection/builders/model_builder_tf2_test.py
```

Now you can convert the dataset you've prepared in the `./training` workspace to TFRecords.
The script to perform this conversion can be copied from [here](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#create-tensorflow-records). Simply save the Python code from the yellow box as a file in your working directory named `generate_tfrecord.py`.

Then, make the following calls from your working directory:
```
python3.6 generate_tfrecord.py -x ./training/images/train -l ./training/annotations/label_map.pbtxt -o ./training/annotations/train.record
python3.6 generate_tfrecord.py -x ./training/images/test -l ./training/annotations/label_map.pbtxt -o ./training/annotations/test.record
```

## 3.3 - Download a pre-trained model from the [TensorFlow Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)

Peruse the Model Zoo and decide which pre-trained model you would like to work with. For our purposes, we will use [SSD MobileNet V2 FPNLite 640x640](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz)

From your working directory, make the following calls:
```
cd training/pre-trained-models/
wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz
tar -xvf ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz
```

## 3.4 - Configure the training job

Copy the extracted model's `pipeline.config` file from its source in `./training/pre-trained-models/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8` to a directory that we'll create in `./training/models`. Let's name this new directory after the model downloaded from the Zoo. Your case may vary, but here are the commands for this example:
```
cd ../..
mkdir training/models/ssd_mobilenet_640x640
cp training/pre-trained-models/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/pipeline.config training/models/ssd_mobilenet_640x640/
```

We will edit the copy of `pipeline.config` to prepare for training on the new dataset. Again, if you have decided to use a different model from the Zoo, then the line numbers in `pipeline.config` you will edit may differ. 

The first thing to do is change the number of classes the new detector will learn. Do this by changing the number (if applicable) following `num_classes:` in line 3 of `pipeline.config`. Next, change the batch size for training, if you want to. This is done by changing the number that follows `batch_size:` on line 135 of `pipeline.config`.

Line 165 has a field named `fine_tune_checkpoint`. Replace the default string `PATH_TO_BE_CONFIGURED` with the path to the checkpoint index file of the original downloaded model. For us, this means entering `"training/pre-trained-models/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/checkpoint/ckpt-0"`. Yes, you should omit the `.index` extension.

Line 171 contains `fine_tune_checkpoint_type:`. Change the value here to `"detection"`.

Next, look for `label_map_path:` on line 175 under `train_input_reader`. Change this value to be `"training/annotations/label_map.pbtxt"`. Find `input_path:` on line 177. Change the following value to `"training/annotations/train.record"`.

Finally, make similar changes to lines 185 and 189: `label_map_path` and `input_path` under the heading `eval_input_reader` should be likewise updated. Set the value for `label_map_path` to the same path to the label map, `"training/annotations/label_map.pbtxt"`, and set the value following `input_path` to `"training/annotations/test.record"`.

That takes care of configuring the training job. We are now ready to train.

## 3.5 - Train

The TFOD API we downloaded includes the script that drives training. Now that the job is configured, we can make a copy of this training script in the working directory so we have it at our fingertips:

```
cp models/research/object_detection/model_main_tf2.py .
```

Begin training by issuing the following command (which you should obviously alter for your specific project):
```
python3.6 model_main_tf2.py --model_dir=training/models/ssd_mobilenet_640x640 --pipeline_config_path=training/models/ssd_mobilenet_640x640/pipeline.config
```

## 3.6 Evaluate a trained network

Training completed, but how well has your network learned? This is actually two concerns; we want to measure both network generalization and per-class accuracy. Validation-set error may have been driven down, but is network identification of some objects weaker than it is for others?

To answer the first question, run the following command:

```
python3 model_main_tf2.py --model_dir=training/models/ssd_mobilenet_640x640 --pipeline_config_path=training/models/ssd_mobilenet_640x640/pipeline.config --checkpoint_dir=training/models/ssd_mobilenet_640x640
```
This tracks performance of your model on the test set found in `./training/annotations/test.record` and collects statistics. It creates a TensorFlow "event" file in `./training/models/ssd_mobilenet_640x640/eval`. If you run TensorBoard, you can see plots of the measurements recorded in these "event" files.

Force-quit execution of `python3 model_main_tf2.py` once you see this message:
```
... Waiting for new checkpoint at training/models/ssd_mobilenet_640x640
```
This script will run indefinitely (occupying your GPU), waiting for a new network checkpoint to appear (presumably trained on another GPU.)

Examine the IoU scores in the print-out or in TensorBoard to decide whether the network has generalized well.

Notice that neither the report printed to screen nor the new "event" file showed us per-class accuracy. To find that out, we will need to run the same script but give it a new `*.config` file at the command line. Only one line of this file needs to change, so issue the following command to make a copy of the existing `*.config` file:
```
cp training/models/ssd_mobilenet_640x640/pipeline.config training/models/ssd_mobilenet_640x640/per_class.config
```

Open the newly-created `per_class.config` in your text editor of choice and find the attribute `metrics_set` inside the attribute `eval_config` (for the MobileNet model we are using, this is on line 181.) Change the value to `"pascal_voc_detection_metrics"` and save.

Now issue the same command as above, but refer the script to this new `per_class.config` file:
```
python3 model_main_tf2.py --model_dir=training/models/ssd_mobilenet_640x640 --pipeline_config_path=training/models/ssd_mobilenet_640x640/per_class.config --checkpoint_dir=training/models/ssd_mobilenet_640x640
```

As before, this prints information to the screen and creates a new "event" file you can view in TensorBoard. The print-out is sufficient to let us know which classes are underperforming. Again, you'll have to force-quit the script once it tells you it's waiting for new checkpoints.

## 3.7 Export a trained network

The TFOD API also includes an exporter script. Let's make a copy of this in the working directory so that we can call it from there.

```
cp models/research/object_detection/exporter_main_v2.py .
```
You may make several models based on specimens in the Model Zoo, so it's a good idea to make one directory for each training inside `./training/exported-models`. Issue the following command to make a folder named `ssd_mobilenet_640x640` for the model we have just trained.

```
mkdir ./training/exported-models/ssd_mobilenet_640x640
```

Now run this script, adjusting the command-line argument values according as necessary for the name you gave your model.

```
python3.6 exporter_main_v2.py --input_type image_tensor --pipeline_config_path training/models/ssd_mobilenet_640x640/pipeline.config --trained_checkpoint_dir training/models/ssd_mobilenet_640x640 --output_directory training/exported-models/ssd_mobilenet_640x640
```

Your working directory should now resemble this:
```
./MyWorkingDirectory
    |
    |--- /Enactment1
    |--- /Enactment2
   ...
    |--- /EnactmentN
    |--- Enactment1_props.txt
    |--- Enactment2_props.txt
   ...
    |--- EnactmentN_props.txt
   ...
    |
    `--- /training
           |
           |--- /annotations
           |       |
           |       |--- label_map.pbtxt
           |       |--- test.record
           |       `--- train.record
           |--- /exported-models
           |       |
           |       `--- /ssd_mobilenet_640x640
           |               |
           |               |--- /checkpoint
           |               |       |
           |               |       |--- checkpoint
           |               |       |--- ckpt-0.data-00000-of-00001
           |               |       `--- ckpt-0.index
           |               |--- /saved_model
           |               |       |
           |               |       |--- /assets
           |               |       |--- /variables
           |               |       |       |
           |               |       |       |--- variables.data-00000-of-00001
           |               |       |       `--- variables.index
           |               |       `--- saved_model.pb
           |               `--- pipeline.config
           |--- /images
           |       |
           |       |--- /train
           |       |       |
           |       |       |--- 1.png
           |       |       |--- 1.xml
           |       |       |--- 2.png
           |       |       |--- 2.xml
           |       |      ...
           |       |
           |       `--- /test
           |               |
           |               |--- 1.png
           |               |--- 1.xml
           |               |--- 2.png
           |               |--- 2.xml
           |              ...
           |
           |--- /models
           |       |
           |       `--- /ssd_mobilenet_640x640
           |               |
           |               |--- /train
           |               |       |
           |               |       |--- events.out.tfevents....  <--- Events files generated by the training process.
           |               |      ...                                 TensorBoard uses these to monitor your progress.
           |               |
           |               |--- checkpoint
           |               |--- ckpt-*.data-00000-of-00001       <--- You'll have two files like these
           |               |--- ckpt-*.index                     <--- for each improved iteration of the model.
           |              ...
           |
           `--- /pre-trained-models
                   |
                   |--- /ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8
                   |       |
                   |      ...                                    <--- The original pre-trained model from the Zoo.
                   |
                   `--- ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz
```

[This tutorial](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#create-tensorflow-records) goes into deeper detail than you'll find here. The dataset used for this research, however, precludes the image labeling and data partitioning that the *Read The Docs* authors describe.
